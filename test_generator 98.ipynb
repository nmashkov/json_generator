{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import json\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = str(pathlib.Path().resolve())\n",
    "TARGET_DIR = ''\n",
    "WORKING_DIR = f'{BASE_DIR}/{TARGET_DIR}'\n",
    "\n",
    "dir_list = os.listdir(WORKING_DIR)\n",
    "\n",
    "mapping = ''\n",
    "mapping_dict = {}\n",
    "\n",
    "i = 1\n",
    "\n",
    "filtered_files = [filename for filename in dir_list if filename.endswith('.xlsx')]\n",
    "mapping_dict = {number + 1: filename for number, filename in enumerate(filtered_files)}\n",
    "\n",
    "#\n",
    "# print(f'BASE_DIR: {BASE_DIR}')\n",
    "# print(f'TARGET_DIR: {TARGET_DIR}')\n",
    "# print(f'WORKING_DIR: {WORKING_DIR}')\n",
    "print(f'dir_list: {dir_list}')\n",
    "\n",
    "if not mapping_dict:\n",
    "    print('\\n==================================================')\n",
    "    print(f'{\"!В папке отсутствует mapping файл формата xlsx!\":^50}')\n",
    "    print('==================================================')\n",
    "elif len(mapping_dict) == 1:\n",
    "    mapping = next(iter(mapping_dict.values()))\n",
    "elif len(mapping_dict) > 1:\n",
    "    for key in mapping_dict.keys():\n",
    "        print(key,': ', mapping_dict[key])\n",
    "        \n",
    "    mapping_key = input('\\nWrite mapping number: ')\n",
    "\n",
    "    mapping = mapping_dict[int(mapping_key)]\n",
    "\n",
    "print(f'mapping: {mapping}')\n",
    "\n",
    "csv = '98_count_all_all_tables.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{WORKING_DIR}/{csv}', header=0, delimiter=';')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values('cnt')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_table_df = df[df['cnt']==0]\n",
    "print(zero_table_df.count())\n",
    "zero_table_lst = zero_table_df['TBL'].unique()\n",
    "print(zero_table_lst.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_df = df[df['cnt']!=0].copy()\n",
    "print(next_df.head())\n",
    "print(next_df.count())\n",
    "next_table_lst = next_df['TBL'].unique()\n",
    "print(next_table_lst.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_df.index = range(1, len(next_df) + 1)\n",
    "next_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_0_100_df = next_df[next_df['cnt']<101]\n",
    "# next_0_100_df.to_csv(f'{WORKING_DIR}/98_0_100_tables.csv')\n",
    "next_0_100_df['TBL'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_101_10000_df = next_df[next_df['cnt'].isin(range(101,10000))]\n",
    "next_101_10000_df.to_csv(f'{WORKING_DIR}/98_101_10000_tables.csv')\n",
    "next_101_10000_df['TBL'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_10001_1000000_df = next_df[next_df['cnt'].isin(range(10001,1000000))]\n",
    "next_10001_1000000_df.to_csv(f'{WORKING_DIR}/98_10001_1000000_tables.csv')\n",
    "next_10001_1000000_df['TBL'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_1000000_end_df = next_df[next_df['cnt']>1000000]\n",
    "next_1000000_end_df.to_csv(f'{WORKING_DIR}/98_1000001_end_tables.csv')\n",
    "next_1000000_end_df['TBL'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(f'{WORKING_DIR}/{mapping}', sheet_name='Mapping',\n",
    "                   usecols=\"D,T:V,Z,AA\")\n",
    "\n",
    "df = df.drop(0,axis=0)\n",
    "\n",
    "df.columns = ['SchemaS', 'SchemaT', 'Table', 'Code', 'Data Type', 'Length']\n",
    "\n",
    "print(df.count())\n",
    "\n",
    "df['Code'] = df['Code'].apply(lambda x: str(x).strip())\n",
    "\n",
    "df = df[df['Code']!='hdp_processed_dttm']\n",
    "print(df.count())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(f'{WORKING_DIR}/{mapping}', sheet_name='Mapping',\n",
    "                   usecols=\"D,T:V,Z,AA\")\n",
    "\n",
    "df = df.drop(0,axis=0)\n",
    "\n",
    "df.columns = ['SchemaS', 'SchemaT', 'Table', 'Code', 'Data Type', 'Length']\n",
    "\n",
    "df = df[df['Code']!='hdp_processed_dttm']\n",
    "\n",
    "df = df.sort_values(['Table'])\n",
    "\n",
    "df.index = range(1, len(df) + 1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_lst = df['Table'].unique()\n",
    "print(\n",
    "    len(tables_lst)\n",
    ")\n",
    "print(tables_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df[df['Table']=='abbreviations_ref'].copy()\n",
    "test_df = test_df.fillna('')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_row = test_df.iloc[0]\n",
    "schema_s = test_row['SchemaS']\n",
    "print(schema_s)\n",
    "schema_t = test_row['SchemaT']\n",
    "print(schema_t)\n",
    "table = test_row['Table']\n",
    "print(table)\n",
    "columns = []\n",
    "columns_casts = []\n",
    "\n",
    "for _, row in test_df.iterrows():\n",
    "    columns.append(row['Code'])\n",
    "    columns_casts.append(\n",
    "        {\n",
    "            \"name\": row['Code'],\n",
    "            \"colType\": row['Data Type']\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(columns)\n",
    "print(columns_casts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_row = test_df.iloc[0]\n",
    "schema_s = test_row['SchemaS']\n",
    "print(schema_s)\n",
    "schema_t = test_row['SchemaT']\n",
    "print(schema_t)\n",
    "table = test_row['Table']\n",
    "print(table)\n",
    "\n",
    "query_full = ''\n",
    "query_prefix = 'select '\n",
    "query_suffix = ' from $schema.$table'\n",
    "query_cast_list = []\n",
    "\n",
    "for _, row in test_df.iterrows():\n",
    "    attr = row['Code']\n",
    "    typ = row['Data Type']\n",
    "    length = f\"({test_row['Length']})\" if test_row['Length'] else ''\n",
    "    query_cast_list.append(f\"cast([{attr}] as {typ}{length} ) as '{attr}'\")\n",
    "\n",
    "query_full = ', '.join(query_cast_list)\n",
    "\n",
    "query_full = query_prefix + query_full + query_suffix\n",
    "\n",
    "print(query_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_flow_entity_lst = []\n",
    "\n",
    "template1 = {\n",
    "    \"loadType\": \"Scd1Replace\",\n",
    "    \"source\": {\n",
    "        \"schema\": schema_s,\n",
    "        \"table\": table,\n",
    "        \"columns\": columns,\n",
    "        \"columnCasts\": columns_casts,\n",
    "        \"jdbcDialect\": \"...\"\n",
    "    },\n",
    "    \"target\": {\n",
    "        \"table\": table\n",
    "    }\n",
    "}\n",
    "\n",
    "test_flow_entity_lst.append(template1)\n",
    "\n",
    "main_json_template = {\n",
    "  \"connection\": {\n",
    "    \"connType\": \"jdbc\",\n",
    "    \"url\": \"...\",\n",
    "    \"driver\": \"...\",\n",
    "    \"user\": \"...\",\n",
    "    \"password\": \"...\"\n",
    "  },\n",
    "  \"commonInfo\": {\n",
    "    \"targetSchema\": schema_t,\n",
    "    \"etlSchema\": schema_t,\n",
    "    \"logsTable\": \"logs597\"\n",
    "  },\n",
    "  \"flows\": test_flow_entity_lst\n",
    "}\n",
    "\n",
    "res_json = json.dumps(main_json_template)\n",
    "\n",
    "print(\n",
    "    res_json.replace('}}', '} }').replace('{{', '{ {')\\\n",
    "            .replace('}]', '} ]').replace('[{', '[ {')\\\n",
    "            .replace(']}', '] }').replace('{[', '{ [')\\\n",
    "            .replace('\"}', '\" }').replace('{\"', '{ \"')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_flow_entity(schema_s, table, columns, columns_casts):\n",
    "\ttemplate = {\n",
    "\t\t\"loadType\": \"Scd1Replace\",\n",
    "\t\t\"source\": {\n",
    "\t\t\t\"schema\": schema_s,\n",
    "\t\t\t\"table\": table,\n",
    "\t\t\t\"columns\": columns,\n",
    "\t\t\t\"columnCasts\": columns_casts,\n",
    "\t\t\t\"jdbcDialect\": \"...\"\n",
    "\t\t},\n",
    "\t\t\"target\": {\n",
    "\t\t\t\"table\": table\n",
    "\t\t}\n",
    "\t}\n",
    "\n",
    "\treturn json.dumps(template)\n",
    "\n",
    "json_str = print_flow_entity(schema_s, table, columns, columns_casts)\n",
    "\n",
    "json_str"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
